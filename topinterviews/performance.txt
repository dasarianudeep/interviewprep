
How do you analyze Web application Performance

1.) Analyze Network Tab - 
Large assets - JS, images
Long running requests - manually defer code exec setTimeout(() => {}, 0), Web Workers, scheduler postTask,
 requestIdleCallback
postTask() allows for finer-grained scheduling of tasks, and is one way to help the 
browser prioritize work so that low priority tasks yield to the main thread
Identify large blocking resources, lazyload
Ensure assets are gzipped
Ensure no duplicate assets are downloaded.

2.) Analyze DOM Content - <=1500
 A large DOM can increase memory usage, cause longer style calculations, and produce costly layout reflows

 3.) Analyze Application Bundles
 webpack-bundle-analyzer
 
 CORE WEB VITALS (CWV) - Standard metrics from Google to measure UX of webpage. It measure different aspects of UX - 
 loading, interactivity and visual stability.
 Field (RUM) - data collected from real users visting the website - CRUX reports, PSI, GSC
 Lab - data collected in a controlled environment with predefined device and network settings - Lighthouse


CLS - Measures how much layout shifts unexpectedly when user lands on your page. (<=0.1)

Causes - 

*Images without dimensions: always include w and h attrs on image/video. or AR. Browser allocates correct space in
the document which loading.
*Ads/embeds/iframes without dimensions: website inserts ad containers dynamically/site resizes ad container
Statically reserve space for ad slot (min-height), placeholders and avoid collapsing space when no ad content is returned
*Dynamic Content: Avoid inserting new content above existing unless in response to user interaction.
If you want to display, reserve sufficent space in viewport in advance.
Replace the old content with new content in fixedsize container / load content offscreen  and show as overlay.
*Web Fonts causing FOIT/FOUT: <link rel="preload" ..../>, font-display: optional
if performance is priority: font-display: optional
If displaying text quickly is a top priority, but you want to still ensure the web-font is used: font-display: swap

LCP - Measures the time from when page starts loading to when large text block or image element is rendered.

1.)TTFB - The time from when the user starts initiating the load until browser receives 1st byte from HTML
2.)RLD - Time diff b/n TTFB and when browser starts loading LCP resource.
3.)RLT - Time it takes to load LCP resource
4.)ERD - Time diff between when LCP resource finishes loading until LCP element is rendered.

Resource Load Delay - 
Goal in this step is to ensure LCP resource starts loading asap.
    Optimise resource discovery - LCP element is <img> element in initial markup. If BG image, preload in HTML markup
If it is dynamically injected via JS(CSR/lazyloading), Preload the image with high priority
<link rel="preload" fetchpriority="high" as="image" href="/path/to/hero-image.webp" type="image/webp">
    Optimise resource priority - 
    <img fetchpriority="high" src="/path/to/hero-image.webp">

Resource Load Time -  reduce size, reduce distance the resource has to travel, eliminate network time entirely
    Reduce size - Serve responsive images
     The webpage shouldn't serve images that are larger than the version that's rendered on viewport.
    <img src="flower-large.jpg" srcset="flower-small.jpg 480w, flower-large.jpg 1080w" sizes="50vw">
                - Serve modern next-gen formats - .webp, .avif
                - Compress images
    Reduce distance - CDNs, image CDNs which also reduce size simultaneously
    Eliminate Network Time - data URL; longer delays due to decode cost

 Element Render Delay - 
 Goal in this step is to render immediately LCP element as soon as it finishes loading.
    Reduce or  inline RB Stylesheets - inline styles or reduce stylesheet size (remove unused CSS, defer non-critical css)
    <link rel="preload" href="styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    Defer or inline scripts - Dont add scripts without async, defer attr in head section
    Reduce JS size - analyse bundles and remove unused modules, replace heavy libraries with light weight alternatives
                    defer JS loading via code splitting on demand


Reduce TTFB - <800ms
    Optimise server processing times, DB queries, avoid multiple page redirects, use CDN to cut response time.

    CDNs - Brotli compression, TLS 1.3 (33% vs 1.2), HTTP/2(multiplexing, header compression), HTTP/3 (HOL blocking, 1.3)
            Minification, Image Optimization



LOAD 3RD PARTY SCRIPTS - 

*Use async,defer attributes on script tags
*Establish early connections - dns-prefetch, preconnect
*Lazy load embedded videos,media
*Avoid scripts that pollute the global scope
*Use CDNs
*Sub resource integrity 
    Subresource Integrity (SRI) is a security feature that enables browsers to verify that resources they
     fetch (for example, from a CDN) are delivered without unexpected manipulation. 
     It works by allowing you to provide a cryptographic hash that a fetched resource must match.

For embedded media -
*Only HTTPS
*Consider sandbox attr on iframe
*CSP

LOAD FONTS - 

*Preconnect to 3rd party origins.
*Use modern WOFF2 format
*font-display: optional
    Informs the browser how it should proceed with text rendering when the associated web font has not loaded.

CRITICAL RENDERING PATH -
Bytes => Characters => Tokens => Nodes => DOM/CSSOM construction
RenderTree - used to compute the layout of each visible element and serves as an input to the paint process that 
            renders the pixels to screen. 



*Adaptive Loading on mobile - connection aware 
*Offline - SW 



----------------------------NEWSFEED-----------------------------------

Fanout on write. With this approach, news feed is pre-computed during write time. 
A new post is delivered to friends’ cache immediately after it is published.

Pros:

The news feed is generated in real-time and can be pushed to friends immediately.

Fetching news feed is fast because the news feed is pre-computed during write time.

Cons:

If a user has many friends, fetching the friend list and generating news feeds for all of them are
 slow and time consuming. It is called hotkey problem.

For inactive users or those rarely log in, pre-computing news feeds waste computing resources.

Fanout on read. The news feed is generated during read time. This is an on-demand model. 
Recent posts are pulled when a user loads her home page.

Pros:

For inactive users or those who rarely log in, fanout on read works better because it will not waste computing resources on them.

Data is not pushed to friends so there is no hotkey problem.

Cons:

Fetching the news feed is slow as the news feed is not pre-computed.


We adopt a hybrid approach to get benefits of both approaches and avoid pitfalls in them.
 Since fetching the news feed fast is crucial, we use a push model for the majority of users.
  For celebrities or users who have many friends/followers, we let followers pull news content on-demand to 
  avoid system overload. 

  ------------------------------------------------------------------------------------------------------------
  IS vs Pagination - 
  IS is best suited when used jus wants to explore with large amount on UGC.
  Pagination is suiteable for goal-oriented apps when the user searches for a specific piece of content.

  IS Pros - User Engagement (reduces bounce rate), Scroll is better than click, offers good UX for  Mobile 
  IS Cons - Performance, SEO, Item search and location, browse endlessely not knowing the end of the content,same
            scrollbar is deceptive, no footer, relatively hard to implement IS than pagination
Pagination Pros - Good conversion, Sense of control, Item Location
Pagination Cons - Extra actions

-----------------------------------------------------------------------------------------------------------------
GraphQL vs REST
1.)GraphQL APIs have a strongly typed schema
2.) Developers often describe the major benefit of GraphQL with the fact that clients can retrieve exactly the data
 they need from the API. They don’t have to rely on REST endpoints that return predefined and fixed responses.
 3.)RAPID Product Iteration - GraphQL exposes a single endpoint that allows you to access multiple resources.
 4.)GraphQL simplifies the task of aggregating data from multiple sources or APIs and then resolving the data 
 to the client in a single API call. On the other hand, API technologies like REST would require multiple HTTP 
 calls to access data from multiple sources
 5.) No Caching in GraphQL
 6.) Error handling in REST is easier when compared to GraphQL.
 7.) N+1 query problem-
 where a client application calls the server N+1 times to fetch one collection resource + N child resources

 ------------------------------------------------ CACHING STRATEGIES ---------------------------------------------
 CACHE-ASIDE: Read Heavy workloads
 Pros -
 *Systems using cache-aside strategy are resilient to cache failures.
 Cons - 
 *Ensuring data consistency is challenging due to the lack of atomic operations on cache and storage.
 *Always a cache miss for the first time and increased latency.

 READ-THROUGH: Read Heavy workloads
 Pros -
 *The application only needs to read from the cache, simplifying the application code.
 Cons - 
 *The system cannot tolerate cache failures, as the cache plays a crucial role in the data retrieval process.
 *The cache and storage systems must share the same data model, limiting the flexibility in handling diff use cases.

 Similar to cache-aside, data consistency can pose challenges in a read-through strategy. 
 If the data in the storage is updated after being cached, inconsistencies may arise.

 WRITE-THROUGH: Write Heavy
 In this write strategy, data is first written to the cache and The cache updates the data in the main database
 Pros -Data is always available in cache and wont result in 'cache miss'.
Cons - Increased write latency.

WRITE-AROUND: 
The application writes data directly to storage system and then 1.)write k,v to cache 2.)invalidate cache 3.)Do ntng (ttl)

WRITE-BACK: Write Heavy
similar to write-through,with the primary diff being that write operations to the storage system are asynchronous.

Pros - Reduced write latency than write-through
Cons - Data inconsistency between cache and database.
If the cache fails before writing data to the storage, the latest updated records may be lost, 
which is not acceptable in some scenarios. To mitigate the risk of data loss due to cache failures,
 cache systems can persist write operations on the cache side. When the cache restarts, 
 it can replay the unfinished operations to recover the cache and write data to the storage system. 

 Cache busting - Specify diff versions of file, fingerprinting or adding query parameters


 Sync vs Async Communication - 
 -----------------------------
 Sync - Low performance,  Tight coupling, Hard to scale when u have high traffic,
 Async - Loosely coupled, Resilient - Decoupled services work independently and failure in one doesn’t cause a
  failure in another. The client of the service won’t be immediately affected. This makes it easier to 
  achieve high-availability of mission-critical systems.

 REST API Design Guidelines -
 ------------------------------ 
 Designing the Spec - how API structure should look like?
 Define endponts, available operations,outling input parameters and output parameters, describing auth methods
 Postman collections
 Documentation

 Versioning - Only when u have a backward-incompatible platform change, API is no more extendable,out-of-date spec
 Swagger - intuitive API Designer, API Console, API Documentation, Mocking Service
 intuitive way to visually design your APIs

 URI versioning, Query parameter, Custom header - Accept-version:v1, Accept header
 Accept: application/vnd.example.v1+json

 API authentication -
 *API key - Simple to implement, identification only, long-lived
 *Oauth2 - Authorization grant flow, client-credentials flow (no user authentication)
 *SAML - open standard for exchanging auth and authorization between IDP and SP.


Desigining the resources - 

 URL's should be nouns, Content-Types

Caching - Cache-Control Header

HTTP Response Code - 1xx - Informational, 2xx - Successful, 3xx - Redirection, 4xx - Client, 5xx - Service

HTTP Error Handling - 
{	
"error":	{	
"code":	400,	
"message":	"The	user	was	missing	required	fields",
"errors":	[{	
"domain":	"global",	
"reason":	"MissingParameter",	
"message":	"User	first	name	cannot	be	empty",	
"locationType":	"parameter",	
"location":	"firstName",	
"extendedHelp":	
"http://docs.domain.ext/users/post"	
},


Securing APIs -  SQL Injection, XSS, DDOS, MITM
Auth
Throttling and rate limiting allow you to prevent abuse of your API, and
ensure that your API withstands large numbers of calls
Validating and Sanitising input parameters
Log request and response activity, Conducting Pen Testing
Practice the principle of least privilege, Encrypt traffic using TLS
Tokenization

CORS
IP Whitelisting
JSON Threat protection - policy to validate a JSON request body by specifying limits for various JSON structures
maxEntries, maxArraySize, maxDepth, maxNameLength, maxValueLength


Analytics - 



-------------------------------------------------------------------------------------------------------------------
Availability - Availability is the percentage of time that some service  is accessible to clients and is operated upon
 under normal conditions.

 Reliability - R measures how the service performs under varying operating conditions.
 MTBF - total Uptime / Total Failures
 MTTR - Total maintenace time / Total Repairs

Scalability is the ability of a system to handle an increasing amount of workload without compromising performance.

Fault tolerance refers to a system’s ability to execute persistently even if one or more of its components fail

How to design for Scale and HA -
--------------------------------
*No SPOF, Design a multi-zone architecture by distribuitng pool of services across mul zones with date replication,
load balancing and automated failover between zones.
*Replicate data across regions for DR i case of regional outage.
*Design mult-region architecture for resilience to regional outages.
*Enable Auto-scaling in VMs, DBs -  sharding.
*Degrade service levels gracefully when overloaded -  Services should detect overload and return lower quality 
responses to the user or partially drop traffic, not fail completely under overload.
*Prevent and mitigate traffic spikes - throttling, queueing, circuit breaking, graceful degradation, prioritizing critical requests.
*Fail safe - If there's a failure due to a problem, the system components should fail in a way that allows the 
overall system to continue to function
*Design API calls and operational commands to be retryable - Idempotent actions
*Minimize Cirtical Dependencies - increase redunancy, use asyn processing(queue, pubsub)
*Ensure that every change can be rolled back


SQL vs NoSQL - 
SQ - Structured Data, Strict Schema, Relational Data, Need for complex joins, TXNS, Date Integrity, Vertical scaling
NoSQL - Schemaless, unstructured or semi-structured data

---------------------------------------WEB SECURITY---------------------------------------------------------------
CORS - 

SOP blocks reading a resource from a different origin.SOP tells the browser to block cross-origin requests.
1.) Browser => Service "Origin" header
2.) Service => Browser "Access-Control-Allow-Origin" header
Share Credentials in CORS
fetch(url, {mode: 'cors', credentials: 'include'});
Access-Control-Allow-Origin must be set to a specific origin (no *) and must set Access-Control-Allow-Credentials to true.

The CORS specification defines a complex request as

*A request that uses methods other than GET, POST, or HEAD
*A request that includes headers other than Accept, Accept-Language or Content-Language
*A request that has a Content-Type header other than application/x-www-form-urlencoded, multipart/form-data, or text/plain

Server response can also include an "Access-Control-Max-Age" header to specify the duration to cache preflight results 

CSRF - 

CSRF Mitigation -
*Accepting only JSON Content-Type - no way for a simple <form> to send JSON
*Disable CORS - Only allow OPTIONS, GET, HEAD
*Check the referrer header
*GET should not have side effects
*CSRF Tokens


Web Sockets vs SSE vs Long-Polling
------------------------------------

WS - allows bi-directional communication between cliet and server. Transmit both binary and text data.
Pros - Reduced resource utilization since persistent connection is established.

Cons - WS do not automatically recover when connections are terminated. 
Some firewalls block WS connections.
Require good upfront work to enable.
No HTTP headers from Server are passed; reduced data payload
You can't leverage caching like HTTP - Intermediate/Edge caching

SSE - Mono-directional(server-to-client) and transported over simple HTTP protocol. Builtin support for reconnection
Event IDs
No firewall issues
Ideal for  live-stocks base apps, Subs to Twitter feed, Receiving live sports score, News update and alternatives

Cons - SSE suffers from a limitation to the maximum  of 6 open connections, which can be especially
 painful when opening multiple tabs, as the limit is per browser and is set to a very low number 
 Lacks extensiblity
 Does not support binary data.


 Long Polling - 
Pros - Simple to implement,based on HTTP. It works in every browser and environment.
Cons - Higher latency, Resource intensive Creates new connection each time, which can be intensive on the server.


MOBILE OPTIMIZATIONS - 

*Your site should have big and easy clickable buttons. 
*Responsive web design
*Compress images
*People use mobile devices of diff screen sizes. When designing a user interface, customize it for diff screen resolutions 
using Media Queries
*You might have elements on your website that won’t convert well to mobile devices - Identify non-mobile friendly feat.
*Disable Pop-ups 
*Never use Flash
*Navigation bars and tab bars take space on the page, and work well when the number of navigation options is small.
*Hamburger menus accommodate a large number of options, but these options are less discoverable.
*Use shorter sentences and therefore shorter paragraphs.
*Use easy-to-use CTA buttons to the bottom of the page.
*Images -choose right image format, fine detail than highes res - PNG or lossless webp, are u optimizing A
photo or a similar image asset - JPEG or lossy Webp 
*Prefer vector formats: vector images are resolution and scale independent, which makes them a perfect fit for the multi-device and high-resolution world.
*Serve scaled images: resize images and ensure that the "display" size is as close as possible to the "natural" size of the image. 
*<img src="flower-large.jpg" srcset="flower-small.jpg 480w, flower-large.jpg 1080w" sizes="50vw">
*Lazy load images in browser 
*Image CDNs specialize in the transformation, optimization, and delivery of images. 
You can also think of them as APIs for accessing and manipulating the images used on your site.
*Consider using a "fetchpriority" attribute value of "high" on the LCP image element so that the browser can
 begin loading that image as soon as possible.
 *If an image is not immediately discoverable in the initial HTML, consider using a rel=preload hint for your
  LCP candidate image so that the browser can load that image ahead of time.


LOAD 3RD PARTY SCRIPTS - 

*Use async,defer attributes on script tags
*Establish early connections - dns-prefetch, preconnect
*Lazy load embedded videos,media
*Avoid scripts that pollute the global scope
*Use CDNs
*Sub resource integrity 
    Subresource Integrity (SRI) is a security feature that enables browsers to verify that resources they
     fetch (for example, from a CDN) are delivered without unexpected manipulation. 
     It works by allowing you to provide a cryptographic hash that a fetched resource must match.

For embedded media -
*Only HTTPS
*Consider sandbox attr on iframe
*CSP
